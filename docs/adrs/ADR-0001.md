---
layout: default
title: ADR-0001 -- Design of the LLM-Play Environment Submission and Testing System
mermaid: true
---

# [ADR](https://github.com/pszjmb1/llm-play/wiki/Architectural-Decision-Records-(ADRs))-0001: Design of the Environment Submission and Testing System

## Context
LLM-Play aims to be a collaborative platform for exploring the capabilities of large language models (LLMs) through reinforcement learning (RL) environments. To support community contributions and ensure consistent, reproducible testing, we need a robust system for submitting and testing RL environments.

The system should:
- Allow users to submit custom RL environments easily.
- Ensure secure execution of submitted environments.
- Provide transparent results and community feedback mechanisms.

## Decision
We will design a **submission and testing system** leveraging the existing **Supabase backend**, **Next.js frontend** architecture, and **Supabase Edge Functions** for handling submissions and task orchestration. The core components will include:

1. **Environment Submission UI:**
   - A submission form at `/submit`, allowing users to upload environment code, descriptions, metadata, and testing parameters.
   - Client and server-side validation to ensure security and data integrity.

2. **Supabase Database Schema:**
   - **Table:** `environments`
     ```sql
     CREATE TABLE environments (
       id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
       user_id uuid REFERENCES auth.users(id),
       name text NOT NULL,
       description text,
       file_url text NOT NULL,
       metadata jsonb,
       status text DEFAULT 'pending',
       created_at timestamp DEFAULT now()
     );
     ```
   - **Table:** `jobs` (for managing test executions)
     ```sql
     CREATE TABLE jobs (
       id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
       environment_id uuid REFERENCES environments(id),
       status text DEFAULT 'queued',
       result jsonb,
       created_at timestamp DEFAULT now(),
       updated_at timestamp
     );
     ```

3. **Supabase Storage:**
   - Submitted environment files will be stored in Supabase Storage, with file URLs saved in the `environments` table.

4. **Supabase Edge Functions for Submission Handling:**
   - Use **Supabase Edge Functions** to process environment submissions and queue jobs for testing.
   - Edge Functions will handle lightweight tasks like validation, status updates, and task dispatching to the testing service.

5. **Testing Service:**
   - A background worker (Node.js or Dockerized service) will:
     - Fetch queued jobs from the `jobs` table.
     - Execute submitted environments in isolated sandboxes.
     - Capture results and update the `jobs` table.

6. **Result Visualization:**
   - A dashboard at `/dashboard` displaying submitted environments, test statuses, and results.
   - Community feedback features (voting, comments) for each environment.

## System Architecture Diagram
<div class="mermaid">
graph TD
    A[User Submits RL Environment] --> B[Next\.js Frontend Submission Form]
    B --> C[Supabase Edge Function]
    C --> D{Validation &amp; Queueing}
    D -->|Valid Submission| E[Supabase Storage --Environment Files--]
    D -->|Queue Job| F[Jobs Table in Supabase]
    F --> G[Background Worker --Node\.js/Docker--]
    G --> H[Execute Environment Test]
    H --> I[Update Job Status in Supabase]
    I --> J[Next\.js Dashboard Displays Results]
    J --> K[Community Feedback --Voting, Comments--]
</div>


## Database Schema Relationship Diagram
<div class="mermaid">
erDiagram
    USERS ||--o{ ENVIRONMENTS : "submits"
    ENVIRONMENTS ||--o{ JOBS : "has"
    
    USERS {
      uuid id PK
      text email
    }
    
    ENVIRONMENTS {
      uuid id PK
      uuid user_id FK
      text name
      text description
      text file_url
      jsonb metadata
      text status
      timestamp created_at
    }
    
    JOBS {
      uuid id PK
      uuid environment_id FK
      text status
      jsonb result
      timestamp created_at
      timestamp updated_at
    }
</div>

## Business Value
- **Community Engagement:** Encourages contributions from researchers and developers by simplifying the submission process.
- **Transparency & Reproducibility:** Ensures that all submitted environments and their test results are openly available, fostering trust within the community.
- **Scalability:** The architecture supports future expansions, such as multi-agent RL environments and complex analytics.
- **Low-Latency Processing:** Using Supabase Edge Functions reduces latency and enhances responsiveness for a global user base.

## Consequences
**Positive:**
- Streamlined submission and testing process will lower the barrier for community participation.
- Secure execution of code reduces risks of malicious submissions.
- Edge Functions simplify deployment and maintenance by integrating with the existing Supabase backend.

**Challenges:**
- Requires ongoing maintenance to manage the testing queue and ensure security in sandboxed environments.
- Edge Functions may have limitations for handling resource-heavy or long-running tasks, necessitating a hybrid architecture with external worker services.

## Alternatives Considered
1. **Manual Review and Testing:**
   - **Rejected** because it limits scalability and places a heavy burden on maintainers.

2. **Third-Party Platforms for Testing (e.g., AWS Lambda):**
   - **Rejected** due to increased complexity and costs associated with integrating external platforms.

3. **Using Only Node.js Background Workers:**
   - **Rejected** in favor of using Supabase Edge Functions for submission handling due to their seamless integration, low-latency, and open-source alignment.

## Effort Estimation (3 Weeks-ish of effort)
### 1. Planning & Environment Setup (1.5 days)

- **Requirements & Architecture:**  
  - Define the core features, security requirements, and integration points.  
  - Sketch out a high‑level architecture that covers the UI, database/storage, edge functions, testing worker, and dashboard.
- **Development Environment:**  
  - Configure basic CI/CD with Github Actions as needed.
- **Justification:**  
  Spending a couple of days up front ensures that subsequent work (especially security and integration) is built on a solid foundation.

### 2. Environment Submission UI (2.5 days)

- **Design & Wireframing (1/4 day):**  
  - Create wireframes or simple mockups for the submission form.
- **Implementation (1-2 days):**  
  - Code the form with all required fields (e.g., file upload, description, etc.) using a modern framework (e.g., React/Next.js).
  - Add client‑side validation and error handling.
- **Testing & Refinement (1/3 day):**  
  - Verify user flows and fix UI bugs. Intergrate unit tests into CI/CD pipleine.
- **Justification:**  
  The UI is the first point of interaction. Even if it’s a single page, getting it right is key for user experience and will serve as the gateway to the backend systems.

### 3. Supabase Integration (Database & Storage) (3.5 days)

- **Database Schema Setup (1/4 day):**  
  - Create migration scripts for the `environments` and `jobs` tables.
- **Client & Server Integration (2 days):**  
  - Write code to interact with Supabase (CRUD operations, file uploads, etc.).
- **Testing (1 day):**  
  - Ensure data is stored and retrieved correctly.
- **Justification:**  
  A secure, well‑defined schema and reliable integration are crucial for a project that executes user-submitted code and stores critical data.  

### 4. Supabase Edge Functions (2.5 days)

- **Function Development (1 day):**  
  - Write edge functions to validate submissions and queue jobs.
- **Integration & Testing (1.5 days):**  
  - Integrate these functions with your UI and storage.
  - Test edge function workflows and error conditions.
- **Justification:**  
  Edge functions are central to offloading tasks securely. Extra time here ensures that you have robust, tested logic to handle potentially sensitive operations. 

### 5. Testing Service / Background Worker (4 days)

- **Design & Prototype (1 day):**  
  - Decide on a secure sandboxing approach (e.g., using Docker).
- **Implementation (2 days):**  
  - Develop a background worker that polls the `jobs` table, simulates environment execution, and updates job status.
- **Testing & Security Review (1 day):**  
  - Validate that the worker runs securely and reliably.
- **Justification:**  
  Even if the worker is a stub at first, ensuring a secure execution environment is complex and requires focused testing and iteration.   

### 6. Dashboard & Result Visualization (2 days)

- **Dashboard UI (1/2 days):**  
  - Build a Next.js page to display submission statuses and results.
- **Community Feedback Features (1/2 day):**  
  - Implement basic voting or comment stubs.
- **Testing & Iteration (1 day):**  
  - Ensure data displays correctly and handle edge cases.
- **Justification:**  
  The dashboard must clearly present data from multiple sources (submissions, job statuses, etc.) to be useful both now and for future community contributions.  

### 7. Security, QA, & Integration Testing (4–5 days)

- **Comprehensive Testing (1/2 day):**  
  - Finalise unit, integration, and end‑to‑end tests across the system.
- **Security Audit (1 day):**  
  - Review the sandboxing, edge functions, and overall data flow for vulnerabilities.
- **Bug Fixes & Refinement (1 day):**  
  - Resolve any issues discovered during testing. Polish as necessary.
- **Justification:**  
  Because the system executes user-supplied code and handles sensitive data, rigorous testing and a security audit are non‑negotiable. This phase ensures the quality and reliability of the final product.

- **Resources:** Currently, the project is driven by one dedicated developer handling frontend, backend, and DevOps responsibilities. As the community grows, additional contributors can join and share these responsibilities.
- **Complexity:** Medium – although AI tools can help automate and streamline many tasks, the project still involves nontrivial challenges such as secure sandboxing and robust integration with Supabase Edge Functions.
- **Dependencies:**
  - Supabase for authentication, storage, database management, and Edge Functions.
  - Docker for secure code execution.
- **Risks:**
  - Potential for security vulnerabilities in executing user-submitted code.
  - Scalability challenges if Edge Functions are pushed beyond their execution limits.

## Status
**Accepted** - This decision aligns with our goals of fostering community engagement, ensuring transparent, reproducible LLM benchmarking, and leveraging Supabase Edge Functions for efficient submission handling.